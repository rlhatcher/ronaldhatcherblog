{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31dcc857",
   "metadata": {},
   "source": [
    "# Manage Data Ingestion\n",
    "\n",
    "General overview\n",
    "\n",
    "- Download content and data to S3 (source data)\n",
    "- Process source data to produce raw data (nnn_kit_lists and nnn_kit_details)\n",
    "- clean and normalise raw data to produce kit_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d1b72",
   "metadata": {},
   "source": [
    "## Libraries and Functions used throughout the notebook\n",
    "\n",
    "I try to keep imports in the scope they are used and only global packages here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env.local')\n",
    "if 'AWS_PROFILE' in os.environ:\n",
    "    del os.environ['AWS_PROFILE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbc69a",
   "metadata": {},
   "source": [
    "### Get HTML Content\n",
    "\n",
    "Retrieves the HTML content of a given URL.\n",
    "\n",
    "Args:\n",
    "\n",
    "- base_url (str): The base URL of the website.\n",
    "- url (str): The specific URL to retrieve the HTML content from.\n",
    "- refresh (bool, optional): If set to True, the HTML content will be retrieved even if it already exists in S3.\n",
    "  - Defaults to False.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- str: The HTML content of the URL, or None if there was an error retrieving the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e73ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import cloudscraper\n",
    "\n",
    "\n",
    "def get_html_content(base_url, url, refresh=False):\n",
    "\n",
    "    # AWS S3 Configuration\n",
    "    aws_bucket_name = os.getenv('AWS_BUCKET_NAME')\n",
    "    aws_region = os.getenv('AWS_REGION')\n",
    "    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "    # Set up S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        region_name=aws_region,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "    # Use the URL as the S3 key\n",
    "    s3_key = url\n",
    "\n",
    "    # Check if the object exists in S3\n",
    "    object_exists = False\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=aws_bucket_name, Key=s3_key)\n",
    "        object_exists = True\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        # Object does not exist or other error\n",
    "        object_exists = False\n",
    "\n",
    "    # If object does not exist or refresh is True, retrieve and store HTML\n",
    "    if not object_exists or refresh:\n",
    "        full_url = base_url + url\n",
    "        scraper = cloudscraper.create_scraper(delay=10, browser='chrome')\n",
    "        html_content = scraper.get(full_url).text\n",
    "\n",
    "        # Store the HTML content in S3\n",
    "        try:\n",
    "            s3_client.put_object(\n",
    "                Bucket=aws_bucket_name, Key=s3_key, Body=html_content, ContentType='text/html')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to S3: {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Retrieve the HTML content from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=aws_bucket_name, Key=s3_key)\n",
    "            html_content = response['Body'].read().decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from S3: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(f\"Retrieved HTML content for {base_url}{url}\")\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ed37d",
   "metadata": {},
   "source": [
    "## Loc Precision Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54d161",
   "metadata": {},
   "source": [
    "### Extract Loc Product List Item\n",
    "\n",
    "Extracts product information from HTML content.\n",
    "\n",
    "Args:\n",
    "\n",
    "- base_url (str): The base URL of the website.\n",
    "- html_content (str): The HTML content to extract product information from.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- list: A list of dictionaries containing product information.\n",
    "\n",
    "Each dictionary contains the following keys:\n",
    "\n",
    "- handle (str): The product handle.\n",
    "- id (str): The product ID.\n",
    "- detail_url (str): The URL of the product detail page.\n",
    "- image_url (str): The URL of the product image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d309c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_products_loc(base_url, html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        product_handle = product.get('data-product-handle')\n",
    "        product_id = product.get('data-product-id')\n",
    "\n",
    "        # Extracting the URL for the product detail page\n",
    "        detail_url = product.find('a', class_='grid-product__link')['href']\n",
    "        full_detail_url = f'{base_url}{detail_url}'\n",
    "\n",
    "        # Extracting the image URL\n",
    "        image_tag = product.find('img', class_='grid__image-contain')\n",
    "        image_url = image_tag.get(\n",
    "            'data-src').replace('{width}', '540') if image_tag else None\n",
    "\n",
    "        product_info = {\n",
    "            'handle': product_handle,\n",
    "            'id': product_id,\n",
    "            'detail_url': full_detail_url,\n",
    "            'image_url': image_url\n",
    "        }\n",
    "\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe79f5",
   "metadata": {},
   "source": [
    "### Extract Loc Product Pages\n",
    "\n",
    "Retrieves all products from a given URL and its subsequent pages.\n",
    "\n",
    "Args:\n",
    "\n",
    "- url (str): The URL of the page to start extracting products from.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- list: A list of product details extracted from the given URL and its subsequent pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_all_products_loc(url):\n",
    "    time.sleep(1)  # Adding a delay\n",
    "\n",
    "    html_content = get_html_content('https://locprecision.com', url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract product details from the current page\n",
    "    products = extract_products_loc('https://locprecision.com', html_content)\n",
    "\n",
    "    # Find the link to the next page\n",
    "    next_span = soup.find('span', class_='next')\n",
    "    if next_span:\n",
    "        next_page = next_span.find('a', title='Next')\n",
    "        if next_page and 'href' in next_page.attrs:\n",
    "            # Recursively extract products from the next page\n",
    "            products += get_all_products_loc(next_page['href'])\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7185bf5",
   "metadata": {},
   "source": [
    "### Write Loc Kit List\n",
    "\n",
    "Write a list of products to a CSV file.\n",
    "\n",
    "Args:\n",
    "\n",
    "- products (list): A list of dictionaries representing products.\n",
    "- filename (str, optional): The name of the CSV file to write to. Defaults to 'loc_kits.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def write_loc_kits(products, filename='loc_kits.csv'):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        fieldnames = ['handle', 'id', 'detail_url', 'image_url']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for product in products:\n",
    "            writer.writerow(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8990533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Comment\n",
    "\n",
    "\n",
    "def extract_json_ld_loc(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        if 'application/ld+json' in comment:\n",
    "            comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "            json_ld_tag = comment_soup.find(\n",
    "                'script', type='application/ld+json')\n",
    "            if json_ld_tag:\n",
    "                return json.loads(json_ld_tag.string)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_product_details(url):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    html_content = get_html_content(f\"{parsed_url.scheme}://{parsed_url.netloc}\", parsed_url.path)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize a dictionary to hold the extracted details\n",
    "    details = {\n",
    "        'Complexity': None,\n",
    "        'Diameter': None,\n",
    "        'Height': None,\n",
    "        'Weight': None,\n",
    "        'Motor Mount': None,\n",
    "        'Parachute Size': None,\n",
    "        'Shock Cord Type': None,\n",
    "        'Shock Cord Mount': None,\n",
    "        'Fin Thickness': None,\n",
    "        'Ring Thickness': None,\n",
    "        'Instructions': None,\n",
    "        'Decal': None,\n",
    "        'Name': None,\n",
    "        'Image URL': None,\n",
    "        'SKU': None,\n",
    "        'Price': None,\n",
    "        'Currency': None,\n",
    "        'Stock Status': None,\n",
    "        'Product URL': None,\n",
    "        'Description': None,\n",
    "        'Links': []\n",
    "    }\n",
    "\n",
    "    # Extract JSON-LD data\n",
    "    json_ld_data = extract_json_ld_loc(html_content)\n",
    "    if json_ld_data:\n",
    "        details.update({\n",
    "            'Name': json_ld_data.get('name'),\n",
    "            'Image URL': json_ld_data.get('image', {}).get('url'),\n",
    "            'SKU': json_ld_data.get('sku'),\n",
    "            'Price': json_ld_data.get('offers', [{}])[0].get('price'),\n",
    "            'Currency': json_ld_data.get('offers', [{}])[0].get('priceCurrency'),\n",
    "            'Stock Status': json_ld_data.get('offers', [{}])[0].get('availability'),\n",
    "            'Product URL': json_ld_data.get('url')\n",
    "        })\n",
    "\n",
    "    # Extract other details from the description\n",
    "    description_div = soup.find(\n",
    "        'div', class_='product-single__description rte')\n",
    "    if description_div:\n",
    "        p_tags = description_div.find_all('p')\n",
    "        pattern = re.compile(r'^(\\w+(?:\\s\\w+){0,2}):\\s*(.+)')\n",
    "        for p_tag in p_tags:\n",
    "            if not p_tag.find():\n",
    "                details['Description'] = p_tag.get_text().strip()\n",
    "            elif not pattern.search(p_tag.get_text()):\n",
    "                # Extract <a> tags\n",
    "                a_tags = p_tag.find_all('a')\n",
    "                for a in a_tags:\n",
    "                    link_info = {'text': a.get_text(), 'href': a['href']}\n",
    "                    details['Links'].append(link_info)\n",
    "            else:\n",
    "                lines = p_tag.get_text(separator='\\n').split('\\n')\n",
    "                for line in lines:\n",
    "                    match = pattern.search(line)\n",
    "                    if match:\n",
    "                        name, value = match.groups()\n",
    "                        details[name] = value\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c601fa3",
   "metadata": {},
   "source": [
    "## Process Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317961e",
   "metadata": {},
   "source": [
    "### Process Loc Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = get_all_products_loc('/collections/rocket-kits')\n",
    "write_loc_kits(all_products)\n",
    "\n",
    "with open('loc_kits.csv', newline='') as csvfile, open('loc_kits_details.csv', 'w', newline='') as outputfile:\n",
    "    reader = csv.DictReader(csvfile, quotechar='\"')\n",
    "    fieldnames = [\n",
    "        'Name', 'Image URL', 'Complexity', 'Diameter', 'Height', 'Weight',\n",
    "        'Motor Mount', 'Parachute Size', 'Shock Cord Type', 'Shock Cord Mount',\n",
    "        'Fin Thickness', 'Ring Thickness', 'Instructions', 'Decal', 'Launch Pad', 'Electronics Bay',\n",
    "        'Price', 'Product URL', 'Currency', 'SKU', 'Stock Status', 'Description', 'Rail Buttons',\n",
    "        'Links', 'Vinyl Decals', 'Tec features', 'Decals', 'Fire Blanket', 'Vinyl Decal', 'Parachute', 'Fin Array', 'Rocksim', 'Parachutes', 'Additional Decals'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames,\n",
    "                            quotechar=\"'\", quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        detail_url = row['detail_url']\n",
    "        details = extract_product_details(detail_url)\n",
    "        writer.writerow(details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
