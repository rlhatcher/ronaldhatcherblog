{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31dcc857",
   "metadata": {},
   "source": [
    "# Manage Source and Raw Data\n",
    "\n",
    "General overview\n",
    "\n",
    "- Download content and data to S3 (source data)\n",
    "- Process source data to produce raw data (nnn_kits and nnn_kits_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d1b72",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import boto3\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "load_dotenv('../.env.local')\n",
    "if 'AWS_PROFILE' in os.environ:\n",
    "    del os.environ['AWS_PROFILE']\n",
    "\n",
    "# AWS S3 Configuration\n",
    "aws_bucket_name = os.getenv('AWS_BUCKET_NAME')\n",
    "aws_region = os.getenv('AWS_REGION')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# Set up S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbc69a",
   "metadata": {},
   "source": [
    "### Get HTML Content\n",
    "\n",
    "Retrieves the HTML content of a given URL using S3 as a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(base_url, url, refresh=False): \n",
    "\n",
    "    # Use the URL as the S3 key\n",
    "    s3_key = url\n",
    "\n",
    "    # Check if the object exists in S3\n",
    "    object_exists = False\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=aws_bucket_name, Key=s3_key)\n",
    "        object_exists = True\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        # Object does not exist or other error\n",
    "        object_exists = False\n",
    "\n",
    "    # If object does not exist or refresh is True, retrieve and store HTML\n",
    "    if not object_exists or refresh:\n",
    "        print(f\"Going to Web for {base_url}{url}\")\n",
    "        full_url = base_url + url\n",
    "        scraper = cloudscraper.create_scraper(delay=10, browser='chrome')\n",
    "        html_content = scraper.get(full_url).text\n",
    "\n",
    "        # Store the HTML content in S3\n",
    "        try:\n",
    "            s3_client.put_object(\n",
    "                Bucket=aws_bucket_name, Key=s3_key, Body=html_content, ContentType='text/html')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to S3: {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Retrieve the HTML content from S3\n",
    "        print(f\"Cache from S3 for {base_url}{url}\")\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=aws_bucket_name, Key=s3_key)\n",
    "            html_content = response['Body'].read().decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from S3: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(f\"Retrieved HTML content for {base_url}{url}\")\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b761f",
   "metadata": {},
   "source": [
    "## Estes Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e8854",
   "metadata": {},
   "source": [
    "### Estes Product Pages\n",
    "iterates through product pages extracting links to product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c12b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_products_estes(url):\n",
    "\n",
    "    html_content = get_html_content('https://help.estesrockets.com', url)\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all('a', class_='article-list-link')\n",
    "    extracted_links = [(link['href'], link.get_text()) for link in links]\n",
    "\n",
    "    next_page = soup.find('a', class_='pagination-next-link')\n",
    "    if next_page and 'href' in next_page.attrs:\n",
    "        extracted_links += get_all_products_estes(next_page['href'])\n",
    "\n",
    "    return extracted_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bda3ca",
   "metadata": {},
   "source": [
    "### Estes Product Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_estes(url):\n",
    "\n",
    "    html_content = get_html_content('https://help.estesrockets.com', url)\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Extract image source URL from the 'article-body' class\n",
    "    image_src = soup.find('div', class_='article-body').find('img')['src']\n",
    "\n",
    "    # Extract description and product URL\n",
    "    article_body = soup.find('div', class_='article-body')\n",
    "    description, product_url = None, None\n",
    "    for p_tag in article_body.find_all('p', recursive=False):\n",
    "        if 'Purchase Link' in p_tag.text:\n",
    "            product_url = p_tag.find('a')['href'] if p_tag.find('a') else None\n",
    "        elif not p_tag.find():\n",
    "            description = p_tag.get_text().strip()\n",
    "\n",
    "    # Extract key features from the product attributes table\n",
    "    features = {}\n",
    "    table = soup.find(\n",
    "        'table', class_='woocommerce-product-attributes shop_attributes')\n",
    "    if table:\n",
    "        for row in table.find_all('tr'):\n",
    "            feature_name = row.find('th').get_text().strip()\n",
    "            feature_value = row.find('td').get_text().strip()\n",
    "            if feature_name:\n",
    "                features[feature_name] = feature_value\n",
    "\n",
    "    # Extract the instructions PDF link\n",
    "    instructions_pdf = soup.find('div', class_='article-attachments').find(\n",
    "        'a')['href'] if soup.find('div', class_='article-attachments') else None\n",
    "\n",
    "    return {\n",
    "        \"description\": description,\n",
    "        \"image_src\": image_src,\n",
    "        \"features\": features,\n",
    "        \"instructions\": instructions_pdf,\n",
    "        \"product_url\": product_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0240135",
   "metadata": {},
   "source": [
    "### Write Estes Raw kit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba370c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_estes_kits(filename, links, fields):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "\n",
    "        writer.writeheader()  # Write the fieldnames as a header\n",
    "        for href, text in links:\n",
    "            match = re.match(r\"(\\d{1,4})\\s*-?\\s*(.*)\", text)\n",
    "            if match:\n",
    "                model = match.group(1)\n",
    "                # Remove leading dash and space, if any\n",
    "                name = match.group(2).lstrip(\"â€“ \").strip()\n",
    "            else:\n",
    "                model = 'Unknown'\n",
    "                name = text\n",
    "            writer.writerow({'URL': href, 'Model': model, 'Name': name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58436734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_estes_kit_details(file_path, output_file_path):\n",
    "\n",
    "    fieldnames = [\n",
    "        \"url\",\n",
    "        \"description\",\n",
    "        \"image_src\",\n",
    "        \"Recommended Engines\",\n",
    "        \"Projected Max Altitude\",\n",
    "        \"Recovery System\",\n",
    "        \"Length\",\n",
    "        \"Diameter\",\n",
    "        \"Estimated Weight\",\n",
    "        \"Estimated Assembly Time\",\n",
    "        \"Fin Materials\",\n",
    "        \"Decal Type\",\n",
    "        \"Launch System\",\n",
    "        \"Launch Rod Size\",\n",
    "        \"instructions\",\n",
    "        \"Construction\",\n",
    "        \"Wingspan\",\n",
    "        \"Age Recommendation\",\n",
    "        \"Launch Rod System\",\n",
    "        \"Recovery\",\n",
    "        \"Fin Material\",\n",
    "        \"Estimated Assembly Weight\"\n",
    "    ]\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile, \\\n",
    "            open(output_file_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(csvfile, quotechar='\"')\n",
    "        writer = csv.DictWriter(\n",
    "            outfile, fieldnames=fieldnames, quotechar=\"'\", quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        for row in reader:\n",
    "            url = row[0]\n",
    "            kit_info = extract_product_estes(url)\n",
    "            # Flatten the 'features' dictionary\n",
    "            flattened_features = {k: v for k,\n",
    "                                  v in kit_info['features'].items()}\n",
    "\n",
    "            # Merge all data into a single dictionary\n",
    "            row_data = {'url': kit_info['product_url'], **flattened_features,\n",
    "                        'description': kit_info['description'], 'image_src': kit_info['image_src'], 'instructions': kit_info['instructions'] }\n",
    "            writer.writerow(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ed37d",
   "metadata": {},
   "source": [
    "## Loc Precision Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54d161",
   "metadata": {},
   "source": [
    "### Loc Product Item\n",
    "\n",
    "Extracts product information from HTML content.\n",
    "\n",
    "Args:\n",
    "\n",
    "- base_url (str): The base URL of the website.\n",
    "- html_content (str): The HTML content to extract product information from.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- list: A list of dictionaries containing product information.\n",
    "\n",
    "Each dictionary contains the following keys:\n",
    "\n",
    "- handle (str): The product handle.\n",
    "- id (str): The product ID.\n",
    "- detail_url (str): The URL of the product detail page.\n",
    "- image_url (str): The URL of the product image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d309c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_products_loc(base_url, html_content):\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        product_handle = product.get('data-product-handle')\n",
    "        product_id = product.get('data-product-id')\n",
    "\n",
    "        # Extracting the URL for the product detail page\n",
    "        detail_url = product.find('a', class_='grid-product__link')['href']\n",
    "        full_detail_url = f'{base_url}{detail_url}'\n",
    "\n",
    "        # Extracting the image URL\n",
    "        image_tag = product.find('img', class_='grid__image-contain')\n",
    "        image_url = image_tag.get(\n",
    "            'data-src').replace('{width}', '540') if image_tag else None\n",
    "\n",
    "        product_info = {\n",
    "            'handle': product_handle,\n",
    "            'id': product_id,\n",
    "            'detail_url': full_detail_url,\n",
    "            'image_url': image_url\n",
    "        }\n",
    "\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe79f5",
   "metadata": {},
   "source": [
    "### Loc Product Pages\n",
    "\n",
    "Retrieves all products from a given URL and its subsequent pages.\n",
    "\n",
    "Args:\n",
    "\n",
    "- url (str): The URL of the page to start extracting products from.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- list: A list of product details extracted from the given URL and its subsequent pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_products_loc(url):\n",
    "\n",
    "    html_content = get_html_content('https://locprecision.com', url)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract product details from the current page\n",
    "    products = extract_products_loc('https://locprecision.com', html_content)\n",
    "\n",
    "    # Find the link to the next page\n",
    "    next_span = soup.find('span', class_='next')\n",
    "    if next_span:\n",
    "        next_page = next_span.find('a', title='Next')\n",
    "        if next_page and 'href' in next_page.attrs:\n",
    "            # Recursively extract products from the next page\n",
    "            products += get_all_products_loc(next_page['href'])\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7185bf5",
   "metadata": {},
   "source": [
    "### Write Loc Kit List\n",
    "\n",
    "Write a list of products to a CSV file.\n",
    "\n",
    "Args:\n",
    "\n",
    "- products (list): A list of dictionaries representing products.\n",
    "- filename (str, optional): The name of the CSV file to write to. Defaults to 'loc_kits.csv'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_loc_kits(products, filename='./data_raw/loc_kits.csv'):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        fieldnames = ['handle', 'id', 'detail_url', 'image_url']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for product in products:\n",
    "            writer.writerow(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8990533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_ld_loc(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        if 'application/ld+json' in comment:\n",
    "            comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "            json_ld_tag = comment_soup.find(\n",
    "                'script', type='application/ld+json')\n",
    "            if json_ld_tag:\n",
    "                return json.loads(json_ld_tag.string)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_details(url):\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    html_content = get_html_content(\n",
    "        f\"{parsed_url.scheme}://{parsed_url.netloc}\", parsed_url.path)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize a dictionary to hold the extracted details\n",
    "    details = {\n",
    "        'Complexity': None,\n",
    "        'Diameter': None,\n",
    "        'Height': None,\n",
    "        'Weight': None,\n",
    "        'Motor Mount': None,\n",
    "        'Parachute Size': None,\n",
    "        'Shock Cord Type': None,\n",
    "        'Shock Cord Mount': None,\n",
    "        'Fin Thickness': None,\n",
    "        'Ring Thickness': None,\n",
    "        'Instructions': None,\n",
    "        'Decal': None,\n",
    "        'Name': None,\n",
    "        'Image URL': None,\n",
    "        'SKU': None,\n",
    "        'Price': None,\n",
    "        'Currency': None,\n",
    "        'Stock Status': None,\n",
    "        'Product URL': None,\n",
    "        'Description': None,\n",
    "        'Links': []\n",
    "    }\n",
    "\n",
    "    # Extract JSON-LD data\n",
    "    json_ld_data = extract_json_ld_loc(html_content)\n",
    "    if json_ld_data:\n",
    "        details.update({\n",
    "            'Name': json_ld_data.get('name'),\n",
    "            'Image URL': json_ld_data.get('image', {}).get('url'),\n",
    "            'SKU': json_ld_data.get('sku'),\n",
    "            'Price': json_ld_data.get('offers', [{}])[0].get('price'),\n",
    "            'Currency': json_ld_data.get('offers', [{}])[0].get('priceCurrency'),\n",
    "            'Stock Status': json_ld_data.get('offers', [{}])[0].get('availability'),\n",
    "            'Product URL': json_ld_data.get('url')\n",
    "        })\n",
    "\n",
    "    # Extract other details from the description\n",
    "    description_div = soup.find(\n",
    "        'div', class_='product-single__description rte')\n",
    "    if description_div:\n",
    "        p_tags = description_div.find_all('p')\n",
    "        pattern = re.compile(r'^(\\w+(?:\\s\\w+){0,2}):\\s*(.+)')\n",
    "        for p_tag in p_tags:\n",
    "            if not p_tag.find():\n",
    "                details['Description'] = p_tag.get_text().strip()\n",
    "            elif not pattern.search(p_tag.get_text()):\n",
    "                # Extract <a> tags\n",
    "                a_tags = p_tag.find_all('a')\n",
    "                for a in a_tags:\n",
    "                    link_info = {'text': a.get_text(), 'href': a['href']}\n",
    "                    details['Links'].append(link_info)\n",
    "            else:\n",
    "                lines = p_tag.get_text(separator='\\n').split('\\n')\n",
    "                for line in lines:\n",
    "                    match = pattern.search(line)\n",
    "                    if match:\n",
    "                        name, value = match.groups()\n",
    "                        details[name] = value\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c601fa3",
   "metadata": {},
   "source": [
    "## Process Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317961e",
   "metadata": {},
   "source": [
    "### Process Loc Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = get_all_products_loc('/collections/rocket-kits')\n",
    "write_loc_kits(all_products)\n",
    "\n",
    "with open('./data_raw/loc_kits.csv', newline='') as csvfile, open('./data_raw/loc_kits_details.csv', 'w', newline='') as outputfile:\n",
    "    reader = csv.DictReader(csvfile, quotechar='\"')\n",
    "    fieldnames = [\n",
    "        'Name', 'Image URL', 'Complexity', 'Diameter', 'Height', 'Weight',\n",
    "        'Motor Mount', 'Parachute Size', 'Shock Cord Type', 'Shock Cord Mount',\n",
    "        'Fin Thickness', 'Ring Thickness', 'Instructions', 'Decal', 'Launch Pad', 'Electronics Bay',\n",
    "        'Price', 'Product URL', 'Currency', 'SKU', 'Stock Status', 'Description', 'Rail Buttons',\n",
    "        'Links', 'Vinyl Decals', 'Tec features', 'Decals', 'Fire Blanket', 'Vinyl Decal', 'Parachute', 'Fin Array', 'Rocksim', 'Parachutes', 'Additional Decals'\n",
    "    ]\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames,\n",
    "                            quotechar=\"'\", quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        detail_url = row['detail_url']\n",
    "        details = extract_product_details(detail_url)\n",
    "        writer.writerow(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03a6b0",
   "metadata": {},
   "source": [
    "### Process Estes Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = get_all_products_estes(\n",
    "    '/hc/en-us/sections/8356411218829-Currently-Manufactured-Rockets')\n",
    "fieldnames = ['URL', 'Model', 'Name']  # Define your fieldnames\n",
    "write_estes_kits('./data_raw/estes_kits.csv', all_links, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316271f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_estes_kit_details('./data_raw/estes_kits.csv', './data_raw/estes_kits_details.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
