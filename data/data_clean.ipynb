{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb619be3",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook outlines the process of cleaning a dataset. The purpose is to prepare the data for further analysis by handling missing values, correcting inconsistencies, and normalizing data.\n",
    "\n",
    "### Goals:\n",
    "\n",
    "- **Understand** the structure of the dataset.\n",
    "- **Identify** and handle missing or inconsistent data.\n",
    "- **Prepare** the dataset for graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166aba7",
   "metadata": {},
   "source": [
    "## Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d92f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c22ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns_by_non_null_count(df, non_null_threshold=10):\n",
    "    \"\"\"\n",
    "    Filter columns in a DataFrame based on a threshold of non-null values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be filtered.\n",
    "    non_null_threshold (int): The minimum number of non-null values required to keep a column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame containing only the columns with non-null values above the threshold.\n",
    "    \"\"\"\n",
    "    filtered_columns = [\n",
    "        col for col in df.columns if df[col].count() > non_null_threshold]\n",
    "    return df[filtered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77b01a",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Here we load the data from its source, which could be a CSV file, database, or other formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ef2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.read_csv('./data_raw/estes_kits_details.csv', quotechar=\"'\")\n",
    "df_l = pd.read_csv('./data_raw/loc_kits_details.csv', quotechar=\"'\")\n",
    "\n",
    "df_manufacturers = pd.read_csv('./data_raw/manufacturers.csv')\n",
    "df_motors = pd.read_csv('./data_raw/rocket_motors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d8bc3",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "In this section, we inspect the dataset for inconsistencies, missing values, and anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f262cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manufacturers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b13b98",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Here we handle missing values, correct data inconsistencies, and normalize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406a12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cols with little data\n",
    "filtered_df_e = filter_columns_by_non_null_count(df_e, 10).copy()\n",
    "filtered_df_l = filter_columns_by_non_null_count(df_l, 10).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ace4a",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "In this section, we perform any necessary transformations such as creating new columns or reshaping the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974667c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_l.rename(columns={\n",
    "    'Image URL': 'image_src',\n",
    "    'Product URL': 'url'},\n",
    "    inplace=True)\n",
    "filtered_df_e['mfgID'] = 'Estes'\n",
    "filtered_df_l['mfgID'] = 'Loc'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "merged_df = pd.concat([filtered_df_e, filtered_df_l], ignore_index=True)\n",
    "\n",
    "merged_df['UniqueID'] = [uuid.uuid4() for _ in range(len(merged_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587c6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = [{'name': 'Loc Precision', 'abbrev': 'Loc'}]\n",
    "\n",
    "new_rows_df = pd.DataFrame(new_rows)\n",
    "df_manufacturers = pd.concat([df_manufacturers, new_rows_df], ignore_index=True)\n",
    "\n",
    "df_manufacturers.rename(columns={'abbrev': 'mfgID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accee1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motors.rename(columns={'manufacturerAbbrev': 'mfgID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e08b3d",
   "metadata": {},
   "source": [
    "Consolidate the motor_samples csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c41d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/cyp3f65x2d5f8rzm6kx2p19w0000gn/T/ipykernel_49553/1272419996.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  aggregated = df.groupby('motorId').apply(lambda x: x[['time', 'thrust']].to_dict('records')).reset_index(name='samples')\n"
     ]
    }
   ],
   "source": [
    "# Load the original thrust_samples.csv\n",
    "df = pd.read_csv('./data_raw/thrust_samples.csv')\n",
    "\n",
    "# Group by motorId and aggregate the samples into a list of dictionaries\n",
    "aggregated = df.groupby('motorId').apply(lambda x: x[['time', 'thrust']].to_dict('records')).reset_index(name='samples')\n",
    "\n",
    "# Serialize the samples into a JSON string\n",
    "aggregated['samples'] = aggregated['samples'].apply(json.dumps)\n",
    "\n",
    "# Save to a new CSV file\n",
    "aggregated.to_csv('./data_clean/thrust_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'motorId': ['id1', 'id1', 'id2', 'id2'],\n",
    "    'time': [0.1, 0.2, 0.1, 0.2],\n",
    "    'thrust': [100, 200, 150, 250]\n",
    "})\n",
    "\n",
    "# Adjusted aggregation logic\n",
    "aggregated = df.groupby('motorId', as_index=False).apply(lambda x: x[['time', 'thrust']].to_dict('records')).reset_index(name='samples')\n",
    "aggregated.columns = ['motorId', 'index', 'samples']  # Adjust column names if necessary\n",
    "aggregated.drop(columns=['index'], inplace=True)  # Drop the intermediate index column\n",
    "\n",
    "# Now 'aggregated' DataFrame contains 'motorId' and 'samples' as columns,\n",
    "# where 'samples' is a list of dictionaries for each 'motorId'\n",
    "print(aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d964f3",
   "metadata": {},
   "source": [
    "## Data Quality Check\n",
    "\n",
    "We re-inspect the dataset to ensure that all cleaning steps have been successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Re-checking the dataset\n",
    "merged_df.info()\n",
    "df_manufacturers.info()\n",
    "df_motors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43e796",
   "metadata": {},
   "source": [
    "## Exporting Cleaned Data\n",
    "\n",
    "Save the cleaned data for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc48f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('./data_clean/kits.csv', index=False)\n",
    "df_manufacturers.to_csv('./data_clean/manufacturers.csv', index=False)\n",
    "df_motors.to_csv('./data_clean/motors.csv', index=False)\n",
    "# new_df.to_csv('estes_kits_details_cleaned.csv', index=False)\n",
    "# print(new_df.info())\n",
    "# Save the cleaned dataset\n",
    "# df.to_csv('path/to/cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a47a6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Summarize the cleaning process, note any limitations, and suggest next steps or further analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
